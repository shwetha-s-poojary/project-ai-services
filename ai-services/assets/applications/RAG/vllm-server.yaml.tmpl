apiVersion: v1
kind: Pod
metadata:
  name: "{{ .AppName }}--vllm-server"
  labels:
    ai-services.io/application: "{{ .AppName }}"
  annotations:
    ai-services.io/model1: BAAI/bge-reranker-v2-m3
    ai-services.io/model2: ibm-granite/granite-embedding-278m-multilingual
    ai-services.io/model3: ibm-granite/granite-3.3-8b-instruct
    ai-services.io/instruct--sypre-cards: "4"
spec:
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 64Gi
    - name: models
      hostPath:
        path: "/var/lib/ai-services/models"
        type: Directory
  containers:
    - name: instruct
      image: icr.io/ibmaiu_internal/ppc64le/dd2/spyre-vllm:v1.0.1
      command: ["/bin/sh", "-c"]
      args: [
          "/home/senuser/container-scripts/vllm_start.sh --served-model-name ibm-granite/granite-3.3-8b-instruct"
      ]
      livenessProbe:
        httpGet:
          path: /health
          port: 8000
        initialDelaySeconds: 600
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      env:
        - name: VLLM_MODEL_PATH
          value: "/models/ibm-granite/granite-3.3-8b-instruct"
        - name: TORCH_SENDNN_CACHE_ENABLE
          value: "1"
        - name: VLLM_SPYRE_REQUIRE_PRECOMPILED_DECODERS
          value: "1"
        - name: VLLM_SPYRE_USE_CB
          value: "1"
        - name: MAX_MODEL_LEN
          value: "32768"
        - name: MAX_BATCH_SIZE
          value: "32"
        {{- /* Check if .env.instruct exists and is a non-empty map */}}
        {{- with .env.instruct }}
          {{- /* If it does, '.' (dot) is now scoped to .env.instruct */}}
          {{- range $k, $v := . }}
        - name: {{ $k }}
          value: "{{ $v }}"
          {{- end }}
        {{- end }}
      resources:
        requests:
          podman.io/device=/dev/vfio: 4
          memory: "200Gi"
        limits:
          memory: "200Gi"
      ports:
        - containerPort: 8000
      volumeMounts:
        - mountPath: /models:Z
          name: models
          readOnly: true
        - mountPath: /dev/shm
          name: dshm
    - name: embedding
      image: icr.io/ppc64le-oss/vllm-ppc64le:0.9.1
      command: ["/bin/sh", "-c"]
      args: [
          "vllm serve /models/ibm-granite/granite-embedding-278m-multilingual --served-model-name ibm-granite/granite-embedding-278m-multilingual --port 8001"
      ]
      livenessProbe:
        httpGet:
          path: /health
          port: 8001
        initialDelaySeconds: 120
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          memory: "10Gi"
        limits:
          memory: "10Gi"
      ports:
        - containerPort: 8001
      volumeMounts:
        - mountPath: /models:Z
          name: models
          readOnly: true
    - name: reranker
      image: icr.io/ppc64le-oss/vllm-ppc64le:0.9.1
      command: ["/bin/sh", "-c"]
      args: [
          "vllm serve /models/BAAI/bge-reranker-v2-m3 --served-model-name BAAI/bge-reranker-v2-m3 --port 8002"
      ]
      livenessProbe:
        httpGet:
          path: /health
          port: 8002
        initialDelaySeconds: 120
        periodSeconds: 30
        timeoutSeconds: 5
        failureThreshold: 3
      resources:
        requests:
          memory: "10Gi"
        limits:
          memory: "10Gi"
      ports:
        - containerPort: 8002
      volumeMounts:
        - mountPath: /models:Z
          name: models
          readOnly: true
